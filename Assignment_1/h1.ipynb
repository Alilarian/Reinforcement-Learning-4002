{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import process_time\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLD_SIZE = (8, 8)\n",
    "START_POSITION = [3, 1]\n",
    "OBSTACLES_POSITIONS = [[0, 6], [1, 6], [2, 6], [2, 2], [3, 2], [4, 2], [6, 4]]\n",
    "GOAL_POSITION = [0, 7]\n",
    "\"\"\"\n",
    "Action mapping:\n",
    "    0: Left\n",
    "    1: Down\n",
    "    2: Right\n",
    "    3: Up\n",
    "\"\"\"\n",
    "ACTIONS = [0, 1, 2, 3]\n",
    "\n",
    "ACTION_PROB = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step(state, action):\n",
    "\n",
    "    if state == GOAL_POSITION:\n",
    "        return  START_POSITION, 100\n",
    "    # if agent hits to an obstacle then it stays on previous state\n",
    "    if state in OBSTACLES_POSITIONS:\n",
    "        return state, 0\n",
    "\n",
    "    if action==0: # left\n",
    "        next_state = np.array(state) + np.array([0, -1])\n",
    "    if action==1: # down\n",
    "        next_state = np.array(state) + np.array([1, 0])\n",
    "    if action==2: # right\n",
    "        next_state = np.array(state) + np.array([0, 1])\n",
    "    if action==3: # up\n",
    "        next_state = np.array(state) + np.array([-1, 0])\n",
    "    \n",
    "    x, y = next_state\n",
    "    # check if agent goes out.\n",
    "    if x < 0 or x >= WORLD_SIZE[0] or y < 0 or y >= WORLD_SIZE[1]:\n",
    "        reward = 0\n",
    "        # stay on previous state\n",
    "        next_state = state\n",
    "    else:\n",
    "        reward = 0\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(discount_factor=0.9, tol=1e-5):\n",
    "    \n",
    "    state_values = np.zeros(WORLD_SIZE, dtype=np.float)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # Iterate over all states\n",
    "        new_value = np.zeros_like(state_values)\n",
    "        for i in range(WORLD_SIZE[0]):\n",
    "            for j in range(WORLD_SIZE[1]):\n",
    "                current_value = state_values[i][j]\n",
    "                for action in ACTIONS:\n",
    "                    (next_i, next_j), reward = one_step([i, j], action)\n",
    "\n",
    "                    new_value[i][j] +=ACTION_PROB * (reward + (discount_factor * state_values[next_i, next_j]))\n",
    "\n",
    "                delta = max(delta, np.absolute(new_value[i][j] - current_value))   \n",
    "        if tol > delta:\n",
    "            break\n",
    "        state_values = new_value\n",
    "        \n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002\t0.003\t0.006\t0.012\t0.017\t0.015\t0.0\t100.001\n",
      "0.002\t0.003\t0.006\t0.019\t0.032\t0.035\t0.0\t32.052\n",
      "0.001\t0.001\t0.0\t0.032\t0.072\t0.11\t0.0\t10.402\n",
      "0.001\t0.001\t0.0\t0.051\t0.145\t0.381\t1.106\t3.777\n",
      "0.001\t0.001\t0.0\t0.051\t0.139\t0.335\t0.755\t1.503\n",
      "0.003\t0.004\t0.011\t0.037\t0.088\t0.214\t0.412\t0.644\n",
      "0.003\t0.005\t0.008\t0.013\t0.0\t0.116\t0.218\t0.303\n",
      "0.003\t0.004\t0.008\t0.014\t0.028\t0.082\t0.14\t0.181\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in np.round(policy_evaluation(),decimals=3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy):\n",
    "        \n",
    "    actions = []\n",
    "    for (i,j), val in np.ndenumerate(policy):\n",
    "            \n",
    "            if [i,j] in OBSTACLES_POSITIONS:\n",
    "                actions.append(\"#\")\n",
    "            elif [i,j] == GOAL_POSITION:\n",
    "                actions.append(\"#\")\n",
    "            elif val == 0:\n",
    "                actions.append(\"←\")\n",
    "            elif val == 1:\n",
    "                actions.append(\"↓\")\n",
    "            elif val == 2:\n",
    "                actions.append(\"→\")\n",
    "            elif val == 3:\n",
    "                actions.append(\"↑\")\n",
    "                \n",
    "    return np.reshape(actions, newshape=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↓\t↓\t↓\t↓\t↓\t↓\t#\t#\n",
      "→\t→\t→\t↓\t↓\t↓\t#\t↑\n",
      "→\t↑\t#\t↓\t↓\t↓\t#\t↑\n",
      "↓\t↓\t#\t→\t→\t→\t→\t↑\n",
      "↓\t↓\t#\t→\t→\t→\t→\t↑\n",
      "→\t→\t→\t→\t→\t→\t→\t↑\n",
      "→\t→\t→\t↑\t#\t→\t→\t↑\n",
      "→\t→\t→\t→\t→\t→\t→\t↑\n",
      "\n",
      "\n",
      "\n",
      "32.959\t36.621\t40.69\t45.211\t50.234\t55.816\t0.0\t129.663\n",
      "36.621\t40.69\t45.211\t50.234\t55.816\t62.017\t0.0\t116.696\n",
      "32.959\t36.621\t0.0\t55.816\t62.017\t68.908\t0.0\t105.027\n",
      "29.663\t32.959\t0.0\t62.017\t68.908\t76.564\t85.072\t94.524\n",
      "32.959\t36.621\t0.0\t55.816\t62.017\t68.908\t76.565\t85.072\n",
      "36.621\t40.69\t45.211\t50.234\t55.816\t62.017\t68.908\t76.565\n",
      "32.959\t36.621\t40.69\t45.211\t0.0\t55.816\t62.017\t68.908\n",
      "29.663\t32.959\t36.621\t40.69\t45.211\t50.234\t55.816\t62.017\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation_version_2(policy, tol=1e-5, discount_factor=0.9):\n",
    "    \n",
    "    state_values = np.zeros(WORLD_SIZE, dtype=np.float)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # Iterate over all states\n",
    "        for i in range(WORLD_SIZE[0]):\n",
    "            for j in range(WORLD_SIZE[1]):\n",
    "                current_value = state_values[i, j]\n",
    "\n",
    "                (next_i, next_j), reward = one_step([i, j], policy[i][j])\n",
    "                \n",
    "                state_values[i, j] = 1 * (reward + (discount_factor * state_values[next_i, next_j]))\n",
    "                \n",
    "                delta = max(delta, np.absolute(current_value - state_values[i, j]))\n",
    "                \n",
    "        if tol > delta:\n",
    "            break\n",
    "        \n",
    "    return np.around(state_values, decimals=3)\n",
    "\n",
    "def policy_improvement(value_from_policy, discount_factor=0.9):\n",
    "\n",
    "    new_policy = np.zeros(WORLD_SIZE, dtype=np.int)\n",
    "\n",
    "    for i in range(WORLD_SIZE[0]):\n",
    "        for j in range(WORLD_SIZE[1]):\n",
    "            \n",
    "            action_value = np.zeros(len(ACTIONS), dtype=np.int)\n",
    "            for action in ACTIONS:\n",
    "                val_s_a = 0\n",
    "                (next_i, next_j), reward = one_step([i, j], action)\n",
    "                \n",
    "                val_s_a =  1 * \\\n",
    "                    (reward + (discount_factor * value_from_policy[next_i, next_j]))\n",
    "            \n",
    "                action_value[action] = val_s_a\n",
    "            \n",
    "            new_policy[i][j] = np.argmax(action_value)\n",
    "                    \n",
    "    return new_policy\n",
    "\n",
    "def policy_iteration():\n",
    "    \n",
    "    value_function = np.zeros(WORLD_SIZE, dtype=np.float)\n",
    "    states_policy = np.zeros(WORLD_SIZE, dtype=np.int)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        temp_val_func = policy_evaluation_version_2(states_policy)\n",
    "        improved_policy = policy_improvement(temp_val_func)\n",
    "    \n",
    "        if np.array_equal(states_policy, improved_policy):\n",
    "            break\n",
    "        else:\n",
    "           states_policy = improved_policy\n",
    "\n",
    "    value_function = temp_val_func\n",
    "    \n",
    "    return value_function, states_policy\n",
    "\n",
    "value, action = policy_iteration()\n",
    "\n",
    "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in print_policy(action)]))\n",
    "print(\"\\n\\n\")\n",
    "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in value]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↓\t↓\t↓\t↓\t↓\t↓\t#\t#\n",
      "→\t→\t→\t↓\t↓\t↓\t#\t↑\n",
      "→\t↑\t#\t↓\t↓\t↓\t#\t↑\n",
      "↓\t↓\t#\t→\t→\t→\t→\t↑\n",
      "↓\t↓\t#\t→\t→\t→\t→\t↑\n",
      "→\t→\t→\t→\t→\t→\t→\t↑\n",
      "→\t→\t→\t↑\t#\t→\t→\t↑\n",
      "→\t→\t→\t→\t→\t→\t→\t↑\n",
      "\n",
      "\n",
      "\n",
      "32.959\t36.621\t40.69\t45.211\t50.234\t55.816\t0.0\t129.663\n",
      "36.621\t40.69\t45.211\t50.234\t55.816\t62.017\t0.0\t116.696\n",
      "32.959\t36.621\t0.0\t55.816\t62.017\t68.908\t0.0\t105.027\n",
      "29.663\t32.959\t0.0\t62.017\t68.908\t76.565\t85.072\t94.524\n",
      "32.959\t36.621\t0.0\t55.816\t62.017\t68.908\t76.565\t85.072\n",
      "36.621\t40.69\t45.211\t50.234\t55.816\t62.017\t68.908\t76.565\n",
      "32.959\t36.621\t40.69\t45.211\t0.0\t55.816\t62.017\t68.908\n",
      "29.663\t32.959\t36.621\t40.69\t45.211\t50.234\t55.816\t62.017\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(discount_factor=0.9, tol=1e-8):\n",
    "    \n",
    "    value_function = np.zeros(WORLD_SIZE, dtype=np.float)\n",
    "    states_policy = np.zeros(WORLD_SIZE, dtype=np.int)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(WORLD_SIZE[0]):\n",
    "            for j in range(WORLD_SIZE[1]):\n",
    "                current_value = value_function[i, j]\n",
    "                \n",
    "                val = np.zeros(len(ACTIONS))\n",
    "                for action in ACTIONS:\n",
    "                    (next_i, next_j), reward = one_step([i, j], action)\n",
    "                \n",
    "                    val[action] =  1 * \\\n",
    "                        (reward + (discount_factor * value_function[next_i, next_j]))\n",
    "\n",
    "                states_policy[i][j] = np.argmax(val)\n",
    "                value_function[i][j] = np.max(val)\n",
    "                delta = max(delta, np.absolute(current_value - value_function[i, j])) \n",
    "\n",
    "        if tol > delta:\n",
    "            break\n",
    "    \n",
    "    return value_function, states_policy\n",
    "\n",
    "value, action = value_iteration()\n",
    "\n",
    "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in print_policy(action)]))\n",
    "print(\"\\n\\n\")\n",
    "print('\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in np.round(value, decimals=3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time during Policy Iteration in seconds: 0.2067340630000002\n"
     ]
    }
   ],
   "source": [
    "policy_iteration_runtimes = []\n",
    "for _ in range(1000):\n",
    "    t1_start = process_time()\n",
    "    policy_iteration()\n",
    "    t1_stop = process_time()\n",
    "    policy_iteration_runtimes.append(t1_stop - t1_start)\n",
    "    \n",
    "print(\"Elapsed time during Policy Iteration in seconds:\", np.average(policy_iteration_runtimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time during Policy Iteration in seconds: 0.2913868874999991\n"
     ]
    }
   ],
   "source": [
    "value_iteration_runtimes = []\n",
    "for _ in range(1000):\n",
    "    t1_start = process_time()\n",
    "    value_iteration()\n",
    "    t1_stop = process_time()\n",
    "    policy_iteration_runtimes.append(t1_stop - t1_start)\n",
    "\n",
    "print(\"Elapsed time during Policy Iteration in seconds:\", np.average(policy_iteration_runtimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f78e400f2134f8d98235cc7bc7d6f526f8dea6d6c24ae1e4afa06d4e087a85f5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
